\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{schulman2015trust,mnih2015human,silver2016mastering}
\citation{williams1991function,williams1992simple,sutton1998reinforcement}
\citation{peters2010relative,van2015learning,fox2015taming,schulman2015trust,montgomery2016guided,nachum2017bridging,nachum2017trust,tangkaratt2017guide,abdolmaleki2018maximum,haarnoja2018soft}
\citation{williams1991function,fox2015taming,schulman2017equivalence,nachum2017bridging,haarnoja2017reinforcement}
\citation{nemirovskii1983problem,beck2003mirror}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{nachum2017improving}
\citation{nemirovskii1983problem,beck2003mirror}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Notation and Problem Setting}{2}{subsection.1.1}}
\newlabel{subsec:notations_and_settings}{{1.1}{2}{Notation and Problem Setting}{subsection.1.1}{}}
\newlabel{subsec:notations_and_settings@cref}{{[subsection][1][1]1.1}{[1][2][]2}}
\newlabel{max_expected_reward}{{1}{2}{Notation and Problem Setting}{equation.1.1}{}}
\newlabel{max_expected_reward@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Policy Mirror Descent}{2}{section.2}}
\newlabel{subsec:revisitTRPO}{{2}{2}{Policy Mirror Descent}{section.2}{}}
\newlabel{subsec:revisitTRPO@cref}{{[section][2][]2}{[1][2][]2}}
\newlabel{eq:max_expected_reward_plus_relative_entropy}{{2}{2}{Policy Mirror Descent}{equation.2.2}{}}
\newlabel{eq:max_expected_reward_plus_relative_entropy@cref}{{[equation][2][]2}{[1][2][]2}}
\citation{owen2013monte}
\citation{nemirovskii1983problem,beck2003mirror}
\citation{kevin2012machine}
\newlabel{eq:pmd}{{3}{3}{Policy Mirror Descent}{equation.2.3}{}}
\newlabel{eq:pmd@cref}{{[equation][3][]3}{[1][2][]3}}
\newlabel{prop:mirrordescent_projection}{{1}{3}{}{prop.1}{}}
\newlabel{prop:mirrordescent_projection@cref}{{[prop][1][]1}{[1][3][]3}}
\newlabel{prop:monoto_policymirrordescent}{{2}{3}{}{prop.2}{}}
\newlabel{prop:monoto_policymirrordescent@cref}{{[prop][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reversed Entropy Policy Mirror Descent}{3}{section.3}}
\newlabel{subsec:repmd}{{3}{3}{Reversed Entropy Policy Mirror Descent}{section.3}{}}
\newlabel{subsec:repmd@cref}{{[section][3][]3}{[1][3][]3}}
\citation{norouzi2016reward}
\citation{nachum2017improving}
\citation{owen2013monte}
\citation{nachum2017improving}
\newlabel{eq:repmd}{{4}{4}{Reversed Entropy Policy Mirror Descent}{equation.3.4}{}}
\newlabel{eq:repmd@cref}{{[equation][4][]4}{[1][3][]4}}
\newlabel{thm:monotonically_increasing_sr_property}{{1}{4}{}{thm.3.1}{}}
\newlabel{thm:monotonically_increasing_sr_property@cref}{{[thm][1][]1}{[1][4][]4}}
\newlabel{eq:SR}{{5}{4}{}{equation.3.5}{}}
\newlabel{eq:SR@cref}{{[equation][5][]5}{[1][4][]4}}
\newlabel{prop:solvableprojection}{{3}{4}{}{prop.3}{}}
\newlabel{prop:solvableprojection@cref}{{[prop][3][]3}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Learning Algorithms}{4}{subsection.3.1}}
\newlabel{eq:pitautauprime}{{6}{4}{Learning Algorithms}{equation.3.6}{}}
\newlabel{eq:pitautauprime@cref}{{[equation][6][]6}{[1][4][]4}}
\citation{nachum2017bridging,haarnoja2018soft,ding2017cold}
\newlabel{alg:repmd}{{1}{5}{Learning Algorithms}{algorithm.1}{}}
\newlabel{alg:repmd@cref}{{[algorithm][1][]1}{[1][5][]5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  The REPMD algorithm}}{5}{algorithm.1}}
\newlabel{lem:repmdgradientestimate}{{1}{5}{}{lem.1}{}}
\newlabel{lem:repmdgradientestimate@cref}{{[lem][1][]1}{[1][4][]5}}
\newlabel{eq:gradient_estimator}{{7}{5}{}{equation.3.7}{}}
\newlabel{eq:gradient_estimator@cref}{{[equation][7][]7}{[1][4][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Behavior of $\text  {SR}(\pi )$}{5}{subsection.3.2}}
\newlabel{subsec:sr}{{3.2}{5}{Behavior of $\SR (\pi )$}{subsection.3.2}{}}
\newlabel{subsec:sr@cref}{{[subsection][2][3]3.2}{[1][5][]5}}
\newlabel{prop:sr}{{4}{5}{}{prop.4}{}}
\newlabel{prop:sr@cref}{{[prop][4][]4}{[1][5][]5}}
\citation{haarnoja2018soft}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Simulation results for the true reward landscape and $\text  {SR}(\pi _\theta )$ with different values of $\tau $ in a bandit setting with 10,000 actions. Each action is represented by a feature $\omega \in \mathbb  {R}$. Let $\Omega = \left ( \omega _1, \dots  , \omega _{10,000} \right )$ be the feature vector. The policy is parameterized by a weight scalar $\theta \in \mathbb  {R}$. The policy is defined by $\text  {softmax}(\Omega ^{\top }\theta )$. True Reward landscape shows the expected reward as a function of $\theta $. The rest figures show \text  {SR}($\pi _\theta $) as a function of $\theta $ with different value of $\tau $.}}{6}{figure.1}}
\newlabel{fig:srsimulation}{{1}{6}{Simulation results for the true reward landscape and $\SR (\pi _\theta )$ with different values of $\tau $ in a bandit setting with 10,000 actions. Each action is represented by a feature $\omega \in \mathbb {R}$. Let $\Omega = \left ( \omega _1, \dots , \omega _{10,000} \right )$ be the feature vector. The policy is parameterized by a weight scalar $\theta \in \mathbb {R}$. The policy is defined by $\text {softmax}(\Omega ^{\top }\theta )$. True Reward landscape shows the expected reward as a function of $\theta $. The rest figures show \SR ($\pi _\theta $) as a function of $\theta $ with different value of $\tau $}{figure.1}{}}
\newlabel{fig:srsimulation@cref}{{[figure][1][]1}{[1][5][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}An Actor-Critic Extension}{6}{section.4}}
\newlabel{subsec:repmd_value}{{4}{6}{An Actor-Critic Extension}{section.4}{}}
\newlabel{subsec:repmd_value@cref}{{[section][4][]4}{[1][5][]6}}
\newlabel{soft-v-and-pi}{{8}{6}{An Actor-Critic Extension}{equation.4.8}{}}
\newlabel{soft-v-and-pi@cref}{{[equation][8][]8}{[1][6][]6}}
\newlabel{eq:trainV}{{9}{6}{An Actor-Critic Extension}{equation.4.9}{}}
\newlabel{eq:trainV@cref}{{[equation][9][]9}{[1][6][]6}}
\citation{lillicrap2015continuous}
\citation{haarnoja2018soft,fujimoto2018addressing}
\citation{brockman2016openai}
\citation{brockman2016openai}
\citation{brockman2016openai,todorov2012mujoco}
\citation{williams1992simple}
\citation{nachum2017improving}
\citation{lillicrap2015continuous}
\citation{fujimoto2018addressing}
\citation{haarnoja2018soft}
\citation{haarnoja2018soft,fujimoto2018addressing}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Results of MENT (red), UREX (green), and REPMD (blue) on synthetic bandit problem and algorithmic tasks. Plots show average reward with standard error during training. Synthetic bandit results averaged over 5 runs. Algorithmic task results averaged over 25 random training runs (5 runs $\times $ 5 random seeds for neural network initialization). X-axis is number of sampled trajectories. }}{7}{figure.2}}
\newlabel{fig:results}{{2}{7}{Results of MENT (red), UREX (green), and REPMD (blue) on synthetic bandit problem and algorithmic tasks. Plots show average reward with standard error during training. Synthetic bandit results averaged over 5 runs. Algorithmic task results averaged over 25 random training runs (5 runs $\times $ 5 random seeds for neural network initialization). X-axis is number of sampled trajectories}{figure.2}{}}
\newlabel{fig:results@cref}{{[figure][2][]2}{[1][7][]7}}
\newlabel{eq:trainQ}{{10}{7}{An Actor-Critic Extension}{equation.4.10}{}}
\newlabel{eq:trainQ@cref}{{[equation][10][]10}{[1][6][]7}}
\newlabel{eq:trainpi}{{11}{7}{An Actor-Critic Extension}{equation.4.11}{}}
\newlabel{eq:trainpi@cref}{{[equation][11][]11}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}}
\newlabel{sec:experiments}{{5}{7}{Experiments}{section.5}{}}
\newlabel{sec:experiments@cref}{{[section][5][]5}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Settings}{7}{subsection.5.1}}
\newlabel{subsec:tasks}{{5.1}{7}{Settings}{subsection.5.1}{}}
\newlabel{subsec:tasks@cref}{{[subsection][1][5]5.1}{[1][7][]7}}
\citation{nachum2017improving}
\citation{nachum2017improving}
\citation{haarnoja2018soft}
\citation{haarnoja2018soft}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Learning curves of DDPG (red), TD3 (yellow), SAC (green) and PMAC (blue) on mujoco tasks. Plots show mean reward with standard error during training, averaged over five different instances with different random seeds. X-axis is millions of environment steps. We observe that PMAC is consistently able to math and, in many cases, outperform all baseline algorithms across all tasks, both in terms of final performance and sample efficiency. }}{8}{figure.3}}
\newlabel{fig:result-mujoco}{{3}{8}{Learning curves of DDPG (red), TD3 (yellow), SAC (green) and PMAC (blue) on mujoco tasks. Plots show mean reward with standard error during training, averaged over five different instances with different random seeds. X-axis is millions of environment steps. We observe that PMAC is consistently able to math and, in many cases, outperform all baseline algorithms across all tasks, both in terms of final performance and sample efficiency}{figure.3}{}}
\newlabel{fig:result-mujoco@cref}{{[figure][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparative Evaluation}{8}{subsection.5.2}}
\citation{montgomery2016guided}
\citation{tangkaratt2017guide}
\citation{abdolmaleki2018maximum}
\citation{haarnoja2018soft}
\citation{montgomery2016guided}
\citation{abdolmaleki2018maximum}
\citation{tangkaratt2017guide,haarnoja2018soft}
\citation{schulman2015trust}
\citation{abdolmaleki2018maximum}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ablation Study by comparing REPMD and PMAC with designed baselines on ReversedAddition and Ant. }}{9}{figure.4}}
\newlabel{fig:ablation}{{4}{9}{Ablation Study by comparing REPMD and PMAC with designed baselines on ReversedAddition and Ant}{figure.4}{}}
\newlabel{fig:ablation@cref}{{[figure][4][]4}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Ablation Study}{9}{subsection.5.3}}
\newlabel{subsec:ablationstudy}{{5.3}{9}{Ablation Study}{subsection.5.3}{}}
\newlabel{subsec:ablationstudy@cref}{{[subsection][3][5]5.3}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{9}{section.6}}
\newlabel{sec:related_work}{{6}{9}{Related Work}{section.6}{}}
\newlabel{sec:related_work@cref}{{[section][6][]6}{[1][9][]9}}
\citation{williams1991function,fox2015taming,nachum2017bridging}
\citation{liu2015finite,thomas2013projected,mahadevan2012sparse}
\citation{peters2010relative,van2015learning}
\citation{peters2007reinforcement,wierstra2008episodic}
\citation{schulman2015trust,schulman2017proximal}
\citation{nachum2017trust}
\citation{nachum2017bridging}
\bibdata{iclr2019_conference}
\bibcite{abdolmaleki2018maximum}{{1}{2018}{{Abdolmaleki et~al.}}{{Abdolmaleki, Springenberg, Tassa, Munos, Heess, and Riedmiller}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{10}{section.7}}
\newlabel{sec:conclusion_and_future_work}{{7}{10}{Conclusion and Future Work}{section.7}{}}
\newlabel{sec:conclusion_and_future_work@cref}{{[section][7][]7}{[1][10][]10}}
\bibcite{beck2003mirror}{{2}{2003}{{Beck \& Teboulle}}{{Beck and Teboulle}}}
\bibcite{brockman2016openai}{{3}{2016}{{Brockman et~al.}}{{Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba}}}
\bibcite{ding2017cold}{{4}{2017}{{Ding \& Soricut}}{{Ding and Soricut}}}
\bibcite{fox2015taming}{{5}{2015}{{Fox et~al.}}{{Fox, Pakman, and Tishby}}}
\bibcite{fujimoto2018addressing}{{6}{2018}{{Fujimoto et~al.}}{{Fujimoto, van Hoof, and Meger}}}
\bibcite{haarnoja2017reinforcement}{{7}{2017}{{Haarnoja et~al.}}{{Haarnoja, Tang, Abbeel, and Levine}}}
\bibcite{haarnoja2018soft}{{8}{2018}{{Haarnoja et~al.}}{{Haarnoja, Zhou, Abbeel, and Levine}}}
\bibcite{hochreiter1997long}{{9}{1997}{{Hochreiter \& Schmidhuber}}{{Hochreiter and Schmidhuber}}}
\bibcite{lillicrap2015continuous}{{10}{2015}{{Lillicrap et~al.}}{{Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}}}
\bibcite{liu2015finite}{{11}{2015}{{Liu et~al.}}{{Liu, Liu, Ghavamzadeh, Mahadevan, and Petrik}}}
\bibcite{mahadevan2012sparse}{{12}{2012}{{Mahadevan \& Liu}}{{Mahadevan and Liu}}}
\bibcite{mnih2015human}{{13}{2015}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.}}}
\bibcite{montgomery2016guided}{{14}{2016}{{Montgomery \& Levine}}{{Montgomery and Levine}}}
\bibcite{kevin2012machine}{{15}{2012}{{Murphy}}{{}}}
\bibcite{nachum2017improving}{{16}{2017{a}}{{Nachum et~al.}}{{Nachum, Norouzi, and Schuurmans}}}
\bibcite{nachum2017bridging}{{17}{2017{b}}{{Nachum et~al.}}{{Nachum, Norouzi, Xu, and Schuurmans}}}
\bibcite{nachum2017trust}{{18}{2017{c}}{{Nachum et~al.}}{{Nachum, Norouzi, Xu, and Schuurmans}}}
\bibcite{nemirovskii1983problem}{{19}{1983}{{Nemirovskii et~al.}}{{Nemirovskii, Yudin, and Dawson}}}
\bibcite{norouzi2016reward}{{20}{2016}{{Norouzi et~al.}}{{Norouzi, Bengio, Jaitly, Schuster, Wu, Schuurmans, et~al.}}}
\bibcite{owen2013monte}{{21}{2013}{{Owen}}{{}}}
\bibcite{peters2007reinforcement}{{22}{2007}{{Peters \& Schaal}}{{Peters and Schaal}}}
\bibcite{peters2010relative}{{23}{2010}{{Peters et~al.}}{{Peters, M{\"u}lling, and Altun}}}
\bibcite{schulman2015trust}{{24}{2015}{{Schulman et~al.}}{{Schulman, Levine, Abbeel, Jordan, and Moritz}}}
\bibcite{schulman2017equivalence}{{25}{2017{a}}{{Schulman et~al.}}{{Schulman, Chen, and Abbeel}}}
\bibcite{schulman2017proximal}{{26}{2017{b}}{{Schulman et~al.}}{{Schulman, Wolski, Dhariwal, Radford, and Klimov}}}
\bibcite{silver2016mastering}{{27}{2016}{{Silver et~al.}}{{Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.}}}
\bibcite{sutton1998reinforcement}{{28}{1998}{{Sutton et~al.}}{{Sutton, Barto, et~al.}}}
\bibcite{tangkaratt2017guide}{{29}{2017}{{Tangkaratt et~al.}}{{Tangkaratt, Abdolmaleki, and Sugiyama}}}
\bibcite{thomas2013projected}{{30}{2013}{{Thomas et~al.}}{{Thomas, Dabney, Giguere, and Mahadevan}}}
\bibcite{todorov2012mujoco}{{31}{2012}{{Todorov et~al.}}{{Todorov, Erez, and Tassa}}}
\bibcite{van2015learning}{{32}{2015}{{Van~Hoof et~al.}}{{Van~Hoof, Peters, and Neumann}}}
\bibcite{wierstra2008episodic}{{33}{2008}{{Wierstra et~al.}}{{Wierstra, Schaul, Peters, and Schmidhuber}}}
\bibcite{williams1992simple}{{34}{1992}{{Williams}}{{}}}
\bibcite{williams1991function}{{35}{1991}{{Williams \& Peng}}{{Williams and Peng}}}
\bibstyle{iclr2019_conference}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs for \cref  {sec:reversed_emtropy_policy_mirror_descent}}{13}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of \cref  {prop:mirrordescent_projection}}{13}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of \cref  {prop:monoto_policymirrordescent}}{13}{subsection.A.2}}
\newlabel{appsec:monoto_policymirrordescent}{{A.2}{13}{Proof of \cref {prop:monoto_policymirrordescent}}{subsection.A.2}{}}
\newlabel{appsec:monoto_policymirrordescent@cref}{{[subappendix][2][2147483647,1]A.2}{[1][13][]13}}
\newlabel{eq:stationary_point_of_expected_reward}{{12}{13}{Proof of \cref {prop:monoto_policymirrordescent}}{equation.A.12}{}}
\newlabel{eq:stationary_point_of_expected_reward@cref}{{[equation][12][2147483647]12}{[1][13][]13}}
\newlabel{eq:fixed_point_of_PMD_iter}{{13}{13}{Proof of \cref {prop:monoto_policymirrordescent}}{equation.A.13}{}}
\newlabel{eq:fixed_point_of_PMD_iter@cref}{{[equation][13][2147483647]13}{[1][13][]13}}
\newlabel{eq:solution_of_project_step}{{14}{13}{Proof of \cref {prop:monoto_policymirrordescent}}{equation.A.14}{}}
\newlabel{eq:solution_of_project_step@cref}{{[equation][14][2147483647]14}{[1][13][]13}}
\newlabel{eq:equivalence_stationary_fixed_point}{{15}{14}{Proof of \cref {prop:monoto_policymirrordescent}}{equation.A.15}{}}
\newlabel{eq:equivalence_stationary_fixed_point@cref}{{[equation][15][2147483647]15}{[1][13][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of \cref  {thm:monotonically_increasing_sr_property}}{14}{subsection.A.3}}
\newlabel{eq:stationary_point_of_sr}{{16}{14}{Proof of \cref {thm:monotonically_increasing_sr_property}}{equation.A.16}{}}
\newlabel{eq:stationary_point_of_sr@cref}{{[equation][16][2147483647]16}{[1][14][]14}}
\newlabel{eq:fixed_point_of_REPMD_iter}{{17}{14}{Proof of \cref {thm:monotonically_increasing_sr_property}}{equation.A.17}{}}
\newlabel{eq:fixed_point_of_REPMD_iter@cref}{{[equation][17][2147483647]17}{[1][14][]14}}
\citation{nachum2017bridging}
\newlabel{eq:solution_of_project_step_repmd}{{18}{15}{Proof of \cref {thm:monotonically_increasing_sr_property}}{equation.A.18}{}}
\newlabel{eq:solution_of_project_step_repmd@cref}{{[equation][18][2147483647]18}{[1][14][]15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Proof of \cref  {prop:solvableprojection}}{15}{subsection.A.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proof of \cref  {prop:sr}}{15}{subsection.A.5}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Details of REPMD Learning}{15}{appendix.B}}
\newlabel{appx:repmd-learning}{{B}{15}{Details of REPMD Learning}{appendix.B}{}}
\newlabel{appx:repmd-learning@cref}{{[appendix][2][2147483647]B}{[1][15][]15}}
\newlabel{lem:opt_pi_ref}{{2}{15}{}{lem.2}{}}
\newlabel{lem:opt_pi_ref@cref}{{[lem][2][2147483647]2}{[1][15][]15}}
\newlabel{eq:pitautauprime}{{19}{15}{}{equation.B.19}{}}
\newlabel{eq:pitautauprime@cref}{{[equation][19][2147483647]19}{[1][15][]15}}
\newlabel{pi_bar_star_tau_tauprime_prop_form}{{21}{15}{Details of REPMD Learning}{equation.B.21}{}}
\newlabel{pi_bar_star_tau_tauprime_prop_form@cref}{{[equation][21][2147483647]21}{[1][15][]15}}
\citation{haarnoja2018soft,fujimoto2018addressing}
\newlabel{eq:importance_sampling_kl}{{B}{16}{Details of REPMD Learning}{equation.B.21}{}}
\newlabel{eq:importance_sampling_kl@cref}{{[appendix][2][2147483647]B}{[1][15][]16}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Details of PMAC Learning}{16}{appendix.C}}
\newlabel{sec:implementationPMAC}{{C}{16}{Details of PMAC Learning}{appendix.C}{}}
\newlabel{sec:implementationPMAC@cref}{{[appendix][3][2147483647]C}{[1][16][]16}}
\newlabel{lem:rmacgradientestimate}{{3}{16}{}{lem.3}{}}
\newlabel{lem:rmacgradientestimate@cref}{{[lem][3][2147483647]3}{[1][16][]16}}
\newlabel{eq:mac_gradient_estimator}{{23}{16}{}{equation.C.23}{}}
\newlabel{eq:mac_gradient_estimator@cref}{{[equation][23][2147483647]23}{[1][16][]16}}
\newlabel{alg:rmac}{{2}{17}{Details of PMAC Learning}{algorithm.2}{}}
\newlabel{alg:rmac@cref}{{[algorithm][2][2147483647]2}{[1][17][]17}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  The PMAC algorithm}}{17}{algorithm.2}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Stochastic Transition Setting}{17}{appendix.D}}
\newlabel{sec:stochasticsetting}{{D}{17}{Stochastic Transition Setting}{appendix.D}{}}
\newlabel{sec:stochasticsetting@cref}{{[appendix][4][2147483647]D}{[1][17][]17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Notations and Settings}{17}{subsection.D.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}REPMD Optimization Problem}{18}{subsection.D.2}}
\newlabel{eq:repmd_stochastic}{{24}{18}{REPMD Optimization Problem}{equation.D.24}{}}
\newlabel{eq:repmd_stochastic@cref}{{[equation][24][2147483647]24}{[1][17][]18}}
\newlabel{lem:opt_pi_ref_stochastic}{{4}{18}{}{lem.4}{}}
\newlabel{lem:opt_pi_ref_stochastic@cref}{{[lem][4][2147483647]4}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Theoretical Analysis}{18}{subsection.D.3}}
\newlabel{thm:monotonically_increasing_sr_property_stochastic}{{2}{18}{}{thm.D.2}{}}
\newlabel{thm:monotonically_increasing_sr_property_stochastic@cref}{{[thm][2][2147483647]2}{[1][18][]18}}
\newlabel{prop:sr_stochastic}{{5}{19}{}{prop.5}{}}
\newlabel{prop:sr_stochastic@cref}{{[prop][5][2147483647]5}{[1][19][]19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Learning}{19}{subsection.D.4}}
\newlabel{eq:importance_sampling_kl_stochastic}{{25}{19}{Learning}{equation.D.25}{}}
\newlabel{eq:importance_sampling_kl_stochastic@cref}{{[equation][25][2147483647]25}{[1][19][]19}}
\citation{todorov2012mujoco}
\citation{hochreiter1997long}
\citation{nachum2017improving}
\newlabel{thm:repmdgradientestimate_stochastic}{{3}{20}{}{thm.D.3}{}}
\newlabel{thm:repmdgradientestimate_stochastic@cref}{{[thm][3][2147483647]3}{[1][20][]20}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Experiments Details}{20}{appendix.E}}
\newlabel{appendix-experiments}{{E}{20}{Experiments Details}{appendix.E}{}}
\newlabel{appendix-experiments@cref}{{[appendix][5][2147483647]E}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Algorithmic and Mujoco Tasks}{20}{subsection.E.1}}
\newlabel{subsec:benchmarks}{{E.1}{20}{Algorithmic and Mujoco Tasks}{subsection.E.1}{}}
\newlabel{subsec:benchmarks@cref}{{[subappendix][1][2147483647,5]E.1}{[1][20][]20}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Action Dimensions of Mujoco Tasks}}{20}{table.1}}
\newlabel{table:mujoco-act-dims}{{1}{20}{Action Dimensions of Mujoco Tasks}{table.1}{}}
\newlabel{table:mujoco-act-dims@cref}{{[table][1][2147483647]1}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Implementation Details}{20}{subsection.E.2}}
\newlabel{subsec:implementation}{{E.2}{20}{Implementation Details}{subsection.E.2}{}}
\newlabel{subsec:implementation@cref}{{[subappendix][2][2147483647,5]E.2}{[1][20][]20}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces REPMD Hyperparameters in Synthetic Bandit}}{21}{table.2}}
\newlabel{table:paras-bandit}{{2}{21}{REPMD Hyperparameters in Synthetic Bandit}{table.2}{}}
\newlabel{table:paras-bandit@cref}{{[table][2][2147483647]2}{[1][20][]21}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces REPMD Hyperparameters in Algorithmic Tasks}}{21}{table.3}}
\newlabel{table:paras-algorithmic}{{3}{21}{REPMD Hyperparameters in Algorithmic Tasks}{table.3}{}}
\newlabel{table:paras-algorithmic@cref}{{[table][3][2147483647]3}{[1][20][]21}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces REPMD Hyperparameters in Mujoco Tasks}}{21}{table.4}}
\newlabel{table:paras-mujoco}{{4}{21}{REPMD Hyperparameters in Mujoco Tasks}{table.4}{}}
\newlabel{table:paras-mujoco@cref}{{[table][4][2147483647]4}{[1][21][]21}}
