
\section{Related Work}
\label{sec:related_work}

The lift-and-project approach is distinct from the previous literature on policy
search, with the exception of a few recent works:
Mirror Descent Guided Policy Search (MDGPS) \citep{montgomery2016guided},
Guide Actor-Critic (GAC) \citep{tangkaratt2017guide},
Maxmimum aposteriori (MPO) \citep{abdolmaleki2018maximum},
and Soft Actor-Critic (SAC) \citep{haarnoja2018soft}.
These approaches also adopt a mirror descent framework,
but differ from the proposed method in key aspects.
%
MDGPS \citep{montgomery2016guided} follows a different learning principle,
using the Lift Step to learn multiple local policies 
(rather than a single policy)
then aligning these with a global policy in the Project Step.
MDGPS also does not include the entropy term in the Lift objective,
which we have found to be essential for exploration. 
%
MPO \citep{abdolmaleki2018maximum} also neglects to add the additional entropy 
term;
\cref{subsec:ablationstudy} shows that entropy regularization with an
appropriate annealing of $\tau'$ significantly improves learning efficiency.
%
Both GAC and SAC use the mode seeking direction of KL divergence in
the Project Step, reversed from the mean seeking direction
we consider here \citep{tangkaratt2017guide,haarnoja2018soft}. 
Additionally, SAC only uses entropy regularization
in the Lift Step, neglecting the proximal relative entropy.
The benefits of regularizing with relative entropy
has been discussed in TRPO \citep{schulman2015trust}
and MPO \citep{abdolmaleki2018maximum},
where it is noted that proximal regularization
significantly improves learning stability.
GAC seeks to match the mean of Gaussian policies under second order approximation in the Project Step,
instead of directly minimizing the KL divergence with gradient descent.
%
Although one might also attempt to interpret ``one-step'' methods
in terms of lift-and-project,
these approaches would obliviously still differ from REPMD,
given that we use different directions of the KL divergence
for the Lift and Project steps respectively. 

Regarding the optimization objective,
several existing methods have considered related approaches,
either by considering (relative) entropy regularization during policy search,
or directly using KL divergence as the target objective. 
As noted in \cref{subsec:revisitTRPO},
REPMD resembles policy gradient methods that maximize expected reward
with an additional entropy regularizer
\citep{williams1991function,fox2015taming,nachum2017bridging}.
Using KL divergence, or Bregman divergences more generally,
as regularizers has also been explored
in \citet{liu2015finite,thomas2013projected,mahadevan2012sparse}.
However, these approaches differ from the proposed method
in important ways.
In particular, they apply regularization to the parameters of the
\emph{linear} approximated value functions, whereas here KL regularization
is applied directly to the policy space.
The literature on relative entropy policy search also uses a similar
KL divergence regularization scheme
\citep{peters2010relative,van2015learning},
but on joint state-action distributions.
Instead of KL divergence, Reward-Weighted Regression (RWR) uses
a log of the correlation between $\pi^*_\tau$ and $\pi_\theta$,
which is then approximated similar to a cross entropy loss
\citep{peters2007reinforcement,wierstra2008episodic}.

TRPO/PPO also has a similar formulation to
\cref{eq:max_expected_reward_plus_relative_entropy}
as a constrained version with a mean seeking KL divergence
\citep{schulman2015trust}. 
Unfortunately, the monotonic improvement guarantee only exists for an
impractical formulation of TRPO.%
%
\footnote{
For the monotonic improvement guarantee, % to hold,
TRPO must use $\KL^{\max} $ rather than the stanard KL divergence.
} 
Here, by contrast,
we use the lift-and-project reformulation to establish a monotonic
improvement guarantee in a simple and direct way.
Our proposed method also includes additional modifications that,
=======
\cite{schulman2015trust,schulman2017proximal}. 
Our proposed method includes additional modifications that,
>>>>>>> e0c8fb2d8e7529f9cc3150fdde9a95a20d888d9d
as shown in \cref{sec:experiments}, significantly improve performance.
UREX also uses the same mean seeking KL divergence for regularization,
which encourages exploration but also complicates the optimization;
as shown in \cref{sec:experiments},
UREX is significantly less efficient than the method proposed here.

Trust PCL adopts the same objective defined in \cref{eq:repmd},
including both entropy and relative entropy regularization
\citep{nachum2017trust}.
However, the policy update strategy is substantially different:
while REPMD uses KL projection, Trust PCL
minimizes a path inconsistency error (inherited from PCL)
between the value and policy along observed trajectories
\citep{nachum2017bridging}.
Although policy optimization by minimizing path inconsistency error
can efficiently utilize off-policy data, this approach loses the 
desirable monotonic improvement guarantee.

In terms of existing theoretical analyses, 
similar monotonic improvement guarantee exists for TRPO, but only for an
impractical formulation.%
%
\footnote{
	For the monotonic improvement guarantee to hold,
	TRPO must use $\KL^{\max} $ rather than the stanard KL divergence.
} 
Here, by contrast,
we use the lift-and-project reformulation to establish a monotonic
improvement guarantee in a simple and direct way.
MPO provides a guarantee on the regularized reward of a non-parametric policy,
but this depends on the assumption that a non-convex optimization problem
can be solved globally.
SAC obtains a similar result with respect to the optimal achievable
Q-values, again relying on an assumption that a non-convex optimization
problem can be solved globally.
We have shown that similar results hold in the case of PMD/REPMD,
but here we have also established something stronger:
When the projection step cannot be efficiently solved to global optimality,
we have shown that PMD/REPMD still preserve stationary points in 
their respective, principled objectives.
MPO and SAC have no such guarantee, as far as we are aware. 

