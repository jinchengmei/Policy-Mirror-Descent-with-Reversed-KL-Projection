\begin{itemize}
	%\item Mirror Descent: Value based; Linear approximation, with a mirror descent type regularization on the parameter \citep{liu2015finite,thomas2013projected,mahadevan2012sparse}. 
	%\item guided policy search via approximate mirror descent: Used as an objective function to align the learned policy with local "teacher" policies. \citep{montgomery2016guided}
%	\item Trust region: Similar idea; Result also holds in ; Then two stage view with reversed projection \cite{schulman2015trust}
	\item Guided AC for continuous control: two-stage optimization; constraint form; Maximizing past trajectory reward. Gaussian policy for $\pi$ and second order approximation for $Q$ function. We directly estimate its gradient based on samples. \citep{tangkaratt2017guide}
	%\item relative entropy policy search: constraint version; Use KL divergence on join distribution; Linear approximation for value function or in RKHS. No entropy, no multiple projection. \citep{peters2010relative,van2015learning}
\end{itemize}





%Using the KL divergence as a "distance" measure 
 %in the literature of guided policy search, but in a fundamentally different way \citep{montgomery2016guided}. 

Several existing methods are similar to our proposed algorithms,
%with the policy mirror descent (PMD) framework, 
by either considering the relative entropy regularizer in policy search/optimization, or using KL divergence as the loss function to optimize the policy. 
As mentioned in \cref{subsec:revisitTRPO}, 
our work is built on a lift-and-project reformulation of the TRPO objective \citep{schulman2015trust}. 
To address the potential drawbacks of TRPO, we further proposed two modifications on it. 
The literature of relative entropy policy search also uses similar KL divergence regularizer \citep{peters2010relative,van2015learning} but mostly on the joint distributions of state and action rather than the policies. 
Approximate mirror descent policy search \citep{montgomery2016guided} uses the KL divergence to align the simple local policies with the global policy, which is fundamentally different from all the other works mentioned heres.
Using KL divergence, or more general Bregman divergence, as a regularizer has also been explored  in \citet{liu2015finite,thomas2013projected,mahadevan2012sparse}. 
However, such regularization is applied to the parameters of the  linear approximated value functions. In this paper, the KL divergence is applied to the policy space as a "distance" measure for polices. 


These related works include reward-weighted regression \citep{peters2007reinforcement,wierstra2008episodic}, maximum a posterior policy optimization \citep{abdolmaleki2018maximum}, and soft actor-critic \citep{haarnoja2018soft}. 

The proposed reversed entropy policy mirror descent (REPMD) method employs a modified version of PMD framework. First, the objective function of PMD is combined with entropy regularizer to enhance further exploration. Second, the projection step applies the \emph{mean seeking} direction of KL divergence instead of the \emph{mean seeking} one used PMD. As shown in both theoretical and practical justification, these two novelties play a very important role in REPMD. 

The Trust PCL method adopts the same objective defined in \cref{eq:repmd}, which includes both entropy and relative entropy regularizer \citep{nachum2017trust}. However, the policy update strategy is different: while REPMD uses KL projection, Trust PCL inherits the same idea from PCL that minimizes path inconsistency error between value and policy for any trajectories \citep{nachum2017bridging}. Although policy optimization by minimizing path inconsistency error can efficiently utilize off-policy data, it is not justified that this method can guarantee policy improvement during learning. On the other hand, the proposed REPMD method can monotonically increase the softmax approximated expected reward, as shown in \cref{sec:repmd}.
