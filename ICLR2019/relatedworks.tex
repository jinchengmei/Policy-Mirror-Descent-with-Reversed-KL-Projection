
\section{Related Work}
\label{sec:related_work}

The lift-and-project approach is distinct from the previous literature on policy
search, with the exception of a few recent works:
Mirror Descent Guided Policy Search (MDGPS) \citep{montgomery2016guided},
Guide Actor-Critic (GAC) \citep{tangkaratt2017guide},
Maxmimum aposteriori (MPO) \citep{abdolmaleki2018maximum},
and Soft Actor-Critic (SAC) \citep{haarnoja2018soft}.
These approaches also adopt a mirror descent framework,
but differ from the proposed method in key aspects.
%
MDGPS \citep{montgomery2016guided} follows a different learning principle,
using the Lift Step to learn multiple local policies 
(rather than a single policy)
then aligning these with a global policy in the Project Step.
MDGPS also does not include the entropy term in the Lift objective,
which we have found to be essential for exploration. 
%
MPO \citep{abdolmaleki2018maximum} also neglects to add the additional entropy 
term;
\cref{subsec:ablationstudy} shows that entropy regularization with an
appropriate annealing of $\tau'$ significantly improves learning efficiency.
%
Both GAC and SAC use the mode seeking direction of KL divergence in
the Project Step, reversed from the mean seeking direction
we consider here \citep{tangkaratt2017guide,haarnoja2018soft}. 
Additionally, SAC only uses entropy regularization
in the Lift Step, neglecting the proximal relative entropy.
The benefits of regularizing with relative entropy
has been discussed in TRPO \citep{schulman2015trust}
and MPO \citep{abdolmaleki2018maximum},
where it is noted that proximal regularization
significantly improves learning stability.
GAC seeks to match the mean of Gaussian policies in the Project Step,
instead of directly minimizing the KL divergence with gradient descent.
%
Although one might also attempt to interpret ``one-step'' methods
in terms of lift-and-project,
these approaches would obliviously still differ from REPMD,
given that we use different directions of the KL divergence
for the Lift and Project steps respectively. 

Regarding the optimization objective,
several existing methods have considered related approaches,
either by considering (relative) entropy regularization during policy search,
or directly using KL divergence as the target objective. 
As noted in \cref{subsec:revisitTRPO},
REPMD resembles policy gradient methods that maximize expected reward
with an additional entropy regularizer
\citep{williams1991function,fox2015taming,nachum2017bridging}.
Using KL divergence, or Bregman divergences more generally,
as regularizers has also been explored
in \citet{liu2015finite,thomas2013projected,mahadevan2012sparse}.
However, these approaches differ from the proposed method
in important ways.
In particular, they apply regularization to the parameters of the
\emph{linear} approximated value functions, whereas here KL regularization
is applied directly to the policy space.
The literature on relative entropy policy search also uses a similar
KL divergence regularization scheme
\citep{peters2010relative,van2015learning},
but on over joint state-action distributions.
Instead of KL divergence, Reward-Weighted Regression (RWR) uses
a log of the correlation between $\pi^*_\tau$ and $\pi_\theta$,
which is then approximated similar to a cross entropy loss
\citep{peters2007reinforcement,wierstra2008episodic}.

TRPO also has a similar formulation to
\cref{eq:max_expected_reward_plus_relative_entropy}
as a constrained version with a mean seeking KL divergence
\citep{schulman2015trust}. 
%However, \cref{eq:max_expected_reward_plus_relative_entropy} uses a different direction of the KL divergence. 
Unfortunately, the monotonic improvement guarantee only exists for an
impractical formulation of TRPO.%
%
\footnote{
For the monotonic improvement guarantee to hold,
TRPO must use $\KL^{\max} $ rather than the stanard KL divergence.
} 
Here, by contrast,
we use the lift-and-project reformulation to establish a monotonic
improvement guarantee in a simple and direct way.
Our proposed method also includes additional modifications that,
as shown in \ref{sec:experiments}, significantly improve performance.
UREX also uses the same mean seeking KL divergence for regularization,
which encourages exploration but also complicates the optimization;
as shown in \ref{sec:experiments},
UREX is significantly less efficient than the method proposed here.

Trust PCL adopts the same objective defined in \cref{eq:repmd},
including both entropy and relative entropy regularization
\citep{nachum2017trust}.
However, the policy update strategy is substantially different:
while REPMD uses KL projection, Trust PCL %inherits the same idea from PCL that
minimizes a path inconsistency error (inherited from PCL)
between the value and policy along observed trajectories
\citep{nachum2017bridging}.
Although policy optimization by minimizing path inconsistency error
can efficiently utilize off-policy data, this approach loses the 
desirable monotonic improvement guarantee.

In terms of the theoretical analyses, 
\begin{itemize}
	\item MPO: (1) guarantee on the regularized reward of a non-parametric policy. (2) A variant of the algorithm? (3) depends on the assumption that the non-convex problem can be solved globally.
	\item SAC: (1) Similar result on Q-value, given that the non-convex problem can be globally solved. (2) SAC proves the globally optimal convergence on Q-value given it can be solved. PMD/REPMD can achieve a similar result. (3) When it cannot be solved, SAC has no fixed point results.
	\item All the methods doesn't say anything about convergence speed.
\end{itemize}

%it is not justified that this method can guarantee policy improvement during learning. On the other hand, the proposed REPMD method can monotonically increase the softmax approximated expected reward, as shown in \cref{sec:repmd}.


%These related works include maximum a posterior policy optimization \citep{abdolmaleki2018maximum}, and soft actor-critic \citep{haarnoja2018soft}. 

%The proposed reversed entropy policy mirror descent (REPMD) method employs a modified version of PMD framework. First, the objective function of PMD is combined with entropy regularizer to enhance further exploration. Second, the projection step applies the \emph{mean seeking} direction of KL divergence instead of the \emph{mean seeking} one used PMD. As shown in both theoretical and practical justification, these two novelties play a very important role in REPMD. 


