

%\subsection{Reversed Entropy Policy Mirror Descent}
\section{Exploratory Conservative Policy Optimization}
\label{subsec:ECPO}

We now propose two modifications to PMD that overcome 
its aforementioned deficiencies.
These two modifications lead to our main proposed algorithm,
Exploratory Conservative Policy Optimization (ECPO),
which retains desirable theoretical properties while achieving superior performance to PMD in practice.
%(We consider some additional refinements to ECPO in the next section.)

The first modification is to add an additional entropy regularizer
to the Lift Step,
to improve the exploration behavior. %of the algorithm. 
The second modification is to use a reversed, \emph{mean seeking} direction
of the KL divergence in the Project Step.
In particular, the ECPO algorithm solves the following 
alternating
optimization problems to update the policy $\pi_{\theta_{t+1}}$
at each iteration:
%
{\small
\begin{equation}
\label{eq:ECPO}
\begin{split}
&\text{\bf (Project)} \quad  \argmin\limits_{\pi_\theta \in \Pi}{\KL(\bm{ \bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta }) }, \\
&\text{\bf (Lift)} \quad \text{where}\ \ \bar{\pi}_{\tau,\tau^{\prime}}^*  =  \argmax\limits_{\pi \in \Delta}{ \ep\limits_{\rho \sim \pi}{  r(\rho) }} \\
&\qquad \qquad \qquad \qquad \qquad \quad {{ - \tau \KL(\pi \| \pi_{\theta_t}) + \bm{\tau^{\prime} \cH(\pi)} }}.
\end{split}
\end{equation}
}
%

The effects of minimizing different orientations of KL divergence are
well known \citep{kevin2012machine}. And it has proved
to be effective in previous work \citep{norouzi2016reward,nachum2017improving}.
In particular, minimizing $\KL(\pi_{\theta}\|q)$
usually underestimates the support of $q$,
since the objective is infinite if $q = 0$ and $\pi_{\theta} > 0$.
Thus, $\pi_\theta$ is driven to $0$ wherever $q=0$.
The problem is that when $q$ changes, $\pi_\theta$ can have zero mass
on trajectories that have non-zero probability under the new $q$,
hence $\pi_\theta$ will never capture this part of $q$,
leading to mode collapse.
By contrast, minimizing $\KL(q\|\pi_{\theta})$ 
is zero-avoiding in $\pi_{\theta}$,
since if $q > 0$ we must ensure $\pi_{\theta} > 0$.
Note that by \cref{eq:pitautauprime}:
(a) the $q$ in our method is nonzero everywhere,
(b) we further add entropy in \cref{eq:ECPO} to avoid $q$ prematurely converging
to a deterministic policy, 
(c) $\KL(q\|\pi_{\theta})$ is zero-avoiding for minimization over
$\pi_{\theta}$.
These ensure that the proposed method does not exhibit the same mode-seeking 
behavior as MD.
As we will see in \cref{sec:experiments},
ECPO outperforms PMD significantly in experimental evaluations.

\subsection{Learning Algorithms}

We now provide practical learning algorithms for \cref{eq:ECPO}.
The Lift Step
has an analytic solution,
{\small
\begin{equation}
	\label{eq:pitautauprime}
	\bar{\pi}_{\tau,\tau^{\prime}}^*(\rho) \triangleq \frac{\refPi(\rho) \exp\left\{ \frac{r(\rho)-\tau^{\prime} \log \refPi(\rho) }{ \tau+\tau^{\prime}} \right\}}{ \sum_{\rho^{\prime}}{\refPi(\rho^{\prime}) \exp\left\{ \frac{r(\rho^{\prime})-\tau^{\prime} \log \refPi(\rho^{\prime})}{ \tau+\tau^{\prime}} \right\} } }.
\end{equation}
}	
 where we choose $\pi_{\theta_t}$ as the reference policy $\refPi$. The Project Step in \cref{eq:ECPO}, $\min\nolimits_{\pi_\theta \in \Pi}{\KL( \bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta ) }$, can be optimized via stochastic gradient descent, given that one can sample trajectories from $\bar{\pi}_{\tau,\tau^{\prime}}^*$. %To see this, note  that
%$$\argmin\nolimits_{\pi_\theta \in \Pi}\KL(\bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta) = \argmin\nolimits_{\pi_\theta \in \Pi} \ep_{\rho \sim \bar{\pi}_{\tau,\tau^{\prime}}^* }  - \log \pi_\theta(\rho)$$. 
The next lemma shows that sampling from $\bar{\pi}_{\tau,\tau^{\prime}}^*$ can be done using self-normalized importance sampling \citep{owen2013monte} when it is possible to draw multiple samples from $\refPi$, following the idea of UREX \citep{nachum2017improving}.   
\begin{lem}
\label{lem:ECPOgradientestimate}
Let $\omega_k = \frac{r(\rho_k) - \tau^{\prime} \log{\refPi(\rho_k)} }{\tau + \tau^{\prime}}$. Given $K$ \emph{i.i.d.} samples $\{\rho_1, \dots, \rho_K\}$ from the \emph{reference policy} $\refPi$, we have the following unbiased gradient estimator,
{\small
\begin{equation}
\label{eq:gradient_estimator}
	\nabla_{\theta} \KL(\bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta)\approx -\sum\limits_{k=1}^K{ \frac{ \exp\left\{ \omega_k \right\} }{ \sum_{j=1}^K{ \exp\left\{ \omega_j \right\}}} \nabla_{\theta} \log{\pi_\theta(\rho_k)} },
\end{equation}
}
\end{lem}
Pesudeocode of the learning algorithm is presented in \cref{alg:ECPO}. Derivation for the analytic solution of the Lift Step and above Lemma as well as other implementation details can be found in \cref{appx:ECPO-learning}.

\begin{algorithm}[t]
	\caption{\label{alg:ECPO}  The ECPO algorithm}
	\begin{algorithmic}[1]
		\INPUT temperature parameters $\tau$ and $\tau'$, number of samples for computing gradient $K$
		\STATE Random initialized $\pi_{\theta}$
		\FOR { $t=1,2,\ldots$ }
		\STATE Set $\refPi = \pi_\theta$
		\REPEAT 
		\STATE Sample a mini-batch of $K$ trajectories from $\refPi$
		\STATE Compute the gradient according to \cref{eq:gradient_estimator}
		\STATE Update $\pi_{\theta}$ by gradient descent
		\UNTIL $t$ reaches maximum of training steps
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsection{Analysis of ECPO}
We now present the theoretical analysis of ECPO. Our first result shows that, with the additional entropy regularizer to the Lift Step $\tau' > 0$, the policy $\bar{\pi}_{\tau,\tau^{\prime}}^*$ still enjoys sublinear regret by particularly designed choice of $\tau$ and $\tau'$, when the policy class is the probabilistic simplex. 

\begin{thm}
\label{thm:sublinear_regret_simplex}
When the policy class is the simplex, by choosing $\tau'=1/\sqrt{T\log{n}}$, and $\tau + \tau' = \sqrt{T} / \sqrt{2\log n}$, (or $\tau'=1/\sqrt{t\log{n}}$, and $\tau + \tau' = \sqrt{t} / \sqrt{2\log n}$), we have,
\begin{equation*}
    \sum\limits_{t=1}^{T}{ \sum\limits_{\rho}{ \pi^*(\rho) r(\rho)}} - \sum\limits_{t=1}^{T}{ \sum\limits_{\rho}{ \pi_t(\rho) r(\rho)}} \le 4\sqrt{T\log n}.
\end{equation*}
where $\pi^*$ is the optimal policy, $\pi_{t}$ is defined by \cref{eq:pitautauprime} with $\pi_{t-1}$ as the reference policy, and $n$ is the total action/trajectory number.
%\begin{equation*}
 %   \pi_{t+1} = \frac{\pi_t \cdot \exp\left\{ \frac{r - \tau^\prime \log{\pi_t}}{\tau + \tau^\prime} \right\}}{ \pi_t^\top \exp\left\{ \frac{r - \tau^\prime \log{\pi_t}}{\tau + \tau^\prime} \right\} }
%\end{equation*}
\end{thm}

Our second result shows that ECPO still enjoys similar desirable properties (\cref{prop:monoto_policymirrordescent}) to PMD
in general settings, with respect to the surrogate reward $\SR(\pi_\theta)$.
%which we analyze further below.

\begin{thm}
\label{thm:monotonically_increasing_sr_property}
Let $\pi_{\theta_{t}}$ denote the policy at step $t$ of the update
sequence.
ECPO satisfies the following properties for an arbitrary parametrization of $\pi$.
\begin{enumerate}
	\item {\bf (Monotonic Improvement)} 
	%The sequence of policies learned by TRPO is guaranteed to be monotonically improved:
	If the Project Step $\KL( \bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta )$ can be globally solved,
then 
	\begin{equation*}
	\SR(\pi_{\theta_{t+1}}) - \SR(\pithetat)\ge 0,
	\end{equation*}
	where
	{\small
	\begin{equation}
	\label{eq:SR}
	\SR(\pi) \triangleq (\tau + \tau^{\prime})\log{ \sum_{\rho}{ \exp\left\{ \frac{r(\rho) + \tau \log{\pi(\rho)} }{\tau + \tau^{\prime}} \right\} }}.
	\end{equation}
	}
	\item {\bf (Fixed Points)} If the Project Step is optimized by gradient descent, then the fixed points of ECPO are the 
	stationary points of $\SR(\pi_\theta)$. 
\end{enumerate}
%Theorem \ref{thm:monotonically_increasing_sr_property} guarantees the monotonic improvement on $\text{SR}(\pi)$, thus implies that 
%the fixed points of ECPO have a correspondence with the stationary point of $\text{SR}(\pi_\theta)$. \todor[]{Why?}
\end{thm}
\noindent \cref{thm:monotonically_increasing_sr_property} only establishes
desirable properties for ECPO with respect to $\SR(\pi_\theta)$,
but not necessarily to $\ep_{\rho\sim \pi_\theta}{r(\rho)}$. However, we can provide
theoretical and empirical evidence that
$\SR(\pi_\theta)$ is a reasonable surrogate 
that can provide good guidance for learning.
In fact, by properly adjusting the two temperature parameters $\tau$ and
$\tau^{\prime}$,
the resulting surrogate objective $\SR(\pi_\theta)$
recovers existing performance measures.

\begin{lem}
\label{lem:sr}
Let $\hat{r}=r-\tau' \log\pi$, $\hat{r}_\infty = \Vert \hat{r} \Vert_\infty$ and $\eta = \tau+\tau'$. For any policy $\pi$ and $\tau\geq 0$, $\tau'\geq 0$, we have
\begin{align*}
\pi\cdot r + \tau' \cH(\pi) \leq \SR(\pi) \leq  \pi\cdot \hat{r} + \frac{1}{2\eta} \ep_{\rho\sim \pi}{\left[\left( \hat{r}(\rho) - \hat{r}_\infty \right)^2 \right]}.
\end{align*}
Furthermore, 
\begin{enumerate}[label=(\roman*)]
	\item  $\SR(\pi) \to \max_{\rho}{r(\rho)}$, as $\tau \to 0, \tau^{\prime} \to 0$.
	\item $\SR(\pi) \to \ep_{\rho \sim \pi}{r(\rho)}$, as $\tau \to \infty, \tau^{\prime} \to 0$. 
\end{enumerate}
\end{lem}

A key question is the feasibility of solving the
Project Step to global optimality.
%It is obvious that when $\pi(\theta) = \theta \in \Delta$
%the Project Step is a convex optimization problem,
%hence can be solved optimally.
As shown in \cref{prop:solvableprojection}, 
for a one-layer-softmax neural network policy,
the Project Step $\KL( \bar{\pi}_{\tau,\tau^{\prime}}^* \| \pi_\theta)$
can also still be solved to global optimality,
affording computational advantages over PMD.

\begin{prop}
\label{prop:solvableprojection}
Assume $\pi_\theta(s) = \softmax(\phi_s^{\top}\theta)$.
Given any $\refPi$,
the Projection Step
$\min\nolimits_{\theta \in \mathbb{R}^d}{\KL(\refPi \| \pi_\theta)}$
is a convex optimization problem in $\theta$.
\end{prop}


%\subsubsection{Behavior of $\SR(\pi)$}
%\subsection{Property of $\SR(\pi)$}
\if
\label{subsec:sr}

%\subsection{Behavior of $\SR(\pi_\theta)$}

\cref{thm:monotonically_increasing_sr_property} only establishes
desirable properties for ECPO with respect to $\SR(\pi_\theta)$,
but not necessarily $r(\rho)$.
In this section we present theoretical and empirical evidence that
$\SR(\pi_\theta)$ is a reasonable surrogate 
that can provide good guidance for learning,
even when targeting desirable behavior with respect to $r(\rho)$.
In fact, by properly adjusting the two temperature parameters $\tau$ and
$\tau^{\prime}$ in ECPO,
the resulting surrogate objective $\SR(\pi_\theta)$
recovers existing performance measures.

\begin{prop}
\label{prop:sr}
$\SR(\pi_\theta)$ satisfies the properties:
\begin{enumerate}[label=(\roman*)]
	\item  $\SR(\pi_\theta) \to \max_{\rho}{r(\rho)}$, as $\tau \to 0, \tau^{\prime} \to 0$.
	\item $\SR(\pi_\theta) \to \ep_{\rho \sim \pi_\theta}{r(\rho)}$, as $\tau \to \infty, \tau^{\prime} \to 0$. 
\end{enumerate}	
\end{prop}

\begin{remk}
Note that $\text{SR}(\pi_\theta)$ also resembles the 
``softmax value function'' in value based RL
\citep{nachum2017bridging,haarnoja2018soft,ding2017cold}.
The standard softmax value can be recovered by
$\text{SR}(\pi_\theta)$ as a special case when $\tau = 0$ or $\tau'=0$. 
	%\todor[]{But \cref{prop:sr} says that $SR(\pi)$ converges to $max_\rho$ when $\tau$ and $\tau^\prime$ are 0?}
\end{remk}

According to \cref{prop:sr}, one should gradually decrease $\tau^{\prime}$
to reduce the level of exploration as sufficient reward landscape information
is collected during the learning process.
Different choices can be made for $\tau$,
depending on the policy constraint set $\Pi$.
Given $\tau^{\prime} \to 0$ and a sufficiently explored reward landscape,
the resulting unconstrained policy must satisfy
$\bar{\pi}_{\tau,\tau^{\prime}}^* \to \pi^*$ as $\tau \to 0$,
where $\pi^*$ is the globally optimal deterministic policy. 
Therefore, in the Project Step, $\pi_\theta$ is obtained by directly
projecting $\pi^*$ into $\Pi$.
When the policy constraint $\Pi$ has nice properties
that support good behavior of KL projection,
such as convexity,
$\pi_\theta$ will achieve good performance.
However, in practice, $\Pi$ is typically non-convex,
and setting $\tau \to 0$ might not work very well,
since directly projecting $\pi^*$ into $\Pi$ will not always lead to
a $\pi_\theta$ with large expected reward.



On the other hand, as $\tau \to \infty$,
the stationary point of $\SR(\pi_\theta)$ will approach the
stationary point of $\sum_{\rho}{ \pi_\theta(\rho) r(\rho) }$.
\cref{fig:srsimulation} shows the a simulation
investigating the behavior of $\SR(\pi_\theta)$ for different $\tau$.
Note that there is a poor local maximum for $\theta <0$,
where naive gradient method will converge to if initialization is
about $\theta < -10$. 
When $\tau = 0.05$, the reward landscape of $\SR(\pi_\theta)$ guides
$\theta$ to converge to the neighbourhood of $0$,
thus helping avoid the poor local maximum.
Later in training, when $\tau$ increases to $5$,
$\SR(\pi_\theta)$ recovers the true expected reward landscape,
ensuring that $\theta$ will converge to a good local (if not the global)
maximum.
While it is hard for a simulation study to be comprehensive,
these results demonstrate how $\SR(\pi_\theta)$ can offer reasonable guidance
for maximizing the true expected reward.
Further investigation on the behavior of $\SR(\pi_\theta)$
is left for future work.
 
\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{./sr_simulation.pdf}
\end{center}
\caption{
Simulation results for the true reward $\pi_\theta^\top r$ and $\SR(\pi_\theta)$
with different values of $\tau$ in a bandit setting with 10,000 actions. Each action is represented by a feature $\omega\in \mathbb{R}$. Let $\Omega = \left( \omega_1, \dots, \omega_{10,000} \right)$ be the feature vector. The policy $\pi_\theta$ is parameterized by a weight scalar $\theta\in \mathbb{R}$ and defined by $\text{softmax}(\Omega^{\top}\theta)$. True reward landscape shows $\pi_\theta^\top r$ as a function of $\theta$. The rest figures show \SR($\pi_\theta$) as a function of $\theta$ with different values of $\tau$.}
\label{fig:srsimulation}
\end{figure*}
\fi

