 %parameterizatio

\section{Policy Mirror Descent}
\label{subsec:revisitTRPO}

\newcommand{\real}{\mathbb{R}}


We first introduce the Policy Mirror Descent (PMD) strategy,
which forms the basis for our algorithms.
%As mentioned in the introduction, our analysis of this and subsequent methods
%focuses on the \emph{non-convex} setting.
Consider the following optimization problem:
given a \emph{reference policy} $\refPi$ (usually the current policy), 
maximize the proximal regularized expected reward,
using relative entropy as the regularizer:
\begin{equation}
\label{eq:max_expected_reward_plus_relative_entropy}
\pi_{\theta} = \argmax\limits_{\pi_\theta \in \Pi} { \ep\limits_{\rho \sim \pi_\theta}{  r(\rho)  - \tau \KL(\pi_\theta \| \refPi) } }.
\end{equation}
Relative entropy has been widely studied in
online learning and optimization
\citep{nemirovskii1983problem,beck2003mirror},
primarily as a component of the mirror descent method. This regularization makes the policy update in a
conservative fashion, by searching policies within the neighbours of the current policy. 
%Observe that when $\refPi$ is the uniform distribution,
%\cref{eq:max_expected_reward_plus_relative_entropy}
%reduces to entropy regularized expected reward.
In practice
$\pi_\theta$ is usually parametrized as a
function of $\theta \in \real^d$ and
$\Pi$ is generally a non-convex set.
Therefore, 
\cref{eq:max_expected_reward_plus_relative_entropy}
is a difficult constrained optimization problem.

One useful way to decompose this optimization
is to consider an alternating lift-and-project procedure
that isolates the different computational challenges.
%
{\small
\begin{equation}
\label{eq:pmd}
\begin{aligned}
&\text{\bf (Project)} \quad \argmin\limits_{\pi_\theta \in \Pi}{\KL( \pi_\theta \| \bar{\pi}_{\tau}^* )}, \\
&\text{\bf (Lift)}  \quad  \text{where}\ \ \bar{\pi}_{\tau}^* =  \argmax\limits_{\pi \in \Delta}{ \ep\limits_{\rho \sim \pi}{  r(\rho)  - \tau \KL(\pi \| \refPi) } }.
\end{aligned}
\end{equation}
}
%

Crucially, the reformulation \cref{eq:pmd} remains equivalent to the
original problem \cref{eq:max_expected_reward_plus_relative_entropy},
in that it preserves the same set of solutions,
as established in \cref{prop:mirrordescent_projection}.

\begin{prop}
\label{prop:mirrordescent_projection}
Given a \emph{reference policy} $\refPi$,
{\small
\begin{equation*}
	\argmax\limits_{\pi_\theta \in \Pi} { \ep\limits_{\rho \sim \pi_\theta}{  r(\rho)  - \tau \KL(\pi_\theta \| \refPi) } } 
 = \argmin\limits_{\pi_\theta \in \Pi}{ \KL(\pi_\theta \| \bar{\pi}_\tau^*) }.
\end{equation*}
}
\end{prop}
Note this result holds even for the non-convex setting. The reformulation \cref{eq:pmd} immediately leads to the PMD algorithm: Lift the current policy $\pi_{\theta_t}$ to $\bar{\pi}_\tau^*$,
then perform multiple steps of gradient descent in the Project Step
to update $\pi_{\theta_{t+1}}$.%
\footnote{
To estimate this gradient one would need to use self-normalized importance
sampling \cite{owen2013monte}.
We omit the details here since PMD is not our main algorithm;
similar techniques can be found in the implementation of ECPO. 
% in cref-tbd.
}
%Note that vanilla gradient descent methods for TRPO can be interpreted as performing only one step gradient descent for the project step. \todor[]{Is it correct?} 

When $\Pi$ is a convex set, one can show that PMD converges to the optimal policy \citep{nemirovskii1983problem,beck2003mirror}. 
The next proposition shows that for general $\Pi$,
PMD still enjoys desirable properties.

\begin{prop}
\label{prop:monoto_policymirrordescent}
Let $\pi_{\theta_{t}}$ denote the policy at step $t$ of
the update sequence.
Then PMD satisfies the following properties for an arbitrary 
parametrization of $\pi$.
\begin{enumerate}
	\item {\bf (Monotonic Improvement)} 
	%The sequence of policies learned by TRPO is guaranteed to be monotonically improved:
	If the Project Step $\min\limits_{\pi_\theta \in \Pi}{\KL( \pi_\theta \| \bar{\pi}_{\tau}^* )}$ can be globally solved, then
	 \begin{equation*}
	\sE_{\rho \sim \pi_{\theta_{t+1}}}{r(\rho)} - \sE_{\rho \sim \pi_{\theta_{t}}}{  r(\rho)} \ge 0.
	\end{equation*}
	\item {\bf (Fixed Points)} If the Project Step is optimized by gradient descent, then the fixed points of PMD are the 
	 stationary points of $\oep_{\rho \sim \pi_\theta}{  r(\rho)}$. 
\end{enumerate}
\end{prop}

Despite these desirable properties, 
\cref{prop:monoto_policymirrordescent}
relies on the condition that the Project Step in PMD is solved to
global optimality.
It is usually not practical to achieve such a stringent requirement
when $\pi_\theta$ is not convex in $\theta$,
limiting the applicability of \cref{prop:monoto_policymirrordescent}.

Another shortcoming of this strategy is 
that PMD typically gets trapped in poor local optima.
Indeed, while the regularizer helps prevent
a large policy update, it also tends to limit exploration.
Moreover, minimizing 
$\KL(\pi_\theta \| \bar{\pi}_\tau^*)$
is known to be \emph{mode seeking} \citep{kevin2012machine},
which can lead to mode collapse during learning.
Once a policy $\bar{\pi}_\tau^*$ has lost important modes,
learning can easily become trapped at a sub-optimal policy.
Unfortunately, at such points,
the relative entropy regularizer does not encourage further exploration.

