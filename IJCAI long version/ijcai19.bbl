\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Abdolmaleki \bgroup \em et al.\egroup
  }{2018}]{abdolmaleki2018maximum}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
  Heess, and Martin Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In {\em ICLR}, 2018.

\bibitem[\protect\citeauthoryear{Beck and Teboulle}{2003}]{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock {\em Operations Research Letters}, 31(3):167--175, 2003.

\bibitem[\protect\citeauthoryear{Brockman \bgroup \em et al.\egroup
  }{2016}]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv:1606.01540}, 2016.

\bibitem[\protect\citeauthoryear{Daniel \bgroup \em et al.\egroup
  }{2012}]{daniel2012hierarchical}
Christian Daniel, Gerhard Neumann, and Jan Peters.
\newblock Hierarchical relative entropy policy search.
\newblock In {\em Artificial Intelligence and Statistics}, pages 273--281,
  2012.

\bibitem[\protect\citeauthoryear{Deisenroth \bgroup \em et al.\egroup
  }{2013}]{deisenroth2013survey}
Marc~Peter Deisenroth, Gerhard Neumann, Jan Peters, et~al.
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends{\textregistered} in Robotics},
  2(1--2):1--142, 2013.

\bibitem[\protect\citeauthoryear{Fox \bgroup \em et al.\egroup
  }{2015}]{fox2015taming}
Roy Fox, Ari Pakman, and Naftali Tishby.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock {\em arXiv preprint arXiv:1512.08562}, 2015.

\bibitem[\protect\citeauthoryear{Fujimoto \bgroup \em et al.\egroup
  }{2018}]{fujimoto2018addressing}
Scott Fujimoto, Herke van Hoof, and Dave Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[\protect\citeauthoryear{Haarnoja \bgroup \em et al.\egroup
  }{2017}]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock {\em arXiv preprint arXiv:1702.08165}, 2017.

\bibitem[\protect\citeauthoryear{Haarnoja \bgroup \em et al.\egroup
  }{2018}]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[\protect\citeauthoryear{Levine}{2018}]{levine2018reinforcement}
Sergey Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock {\em arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[\protect\citeauthoryear{Lillicrap \bgroup \em et al.\egroup
  }{2015}]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup
  }{2015}]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem[\protect\citeauthoryear{Montgomery and
  Levine}{2016}]{montgomery2016guided}
William~H Montgomery and Sergey Levine.
\newblock Guided policy search via approximate mirror descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4008--4016, 2016.

\bibitem[\protect\citeauthoryear{Murphy}{2012}]{kevin2012machine}
Kevin~P Murphy.
\newblock {\em Machine learning: a probabilistic perspective}.
\newblock Cambridge, MA, 2012.

\bibitem[\protect\citeauthoryear{Nachum \bgroup \em et al.\egroup
  }{2017a}]{nachum2017improving}
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans.
\newblock Improving policy gradient by exploring under-appreciated rewards.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Nachum \bgroup \em et al.\egroup
  }{2017b}]{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2772--2782, 2017.

\bibitem[\protect\citeauthoryear{Nachum \bgroup \em et al.\egroup
  }{2017c}]{nachum2017trust}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Trust-pcl: An off-policy trust region method for continuous control.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Nemirovskii \bgroup \em et al.\egroup
  }{1983}]{nemirovskii1983problem}
Arkadii Nemirovskii, David~Borisovich Yudin, and Edgar~Ronald Dawson.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[\protect\citeauthoryear{Neu \bgroup \em et al.\egroup
  }{2017}]{neu2017unified}
Gergely Neu, Anders Jonsson, and Vicen{\c{c}} G{\'o}mez.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock {\em arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[\protect\citeauthoryear{Norouzi \bgroup \em et al.\egroup
  }{2016}]{norouzi2016reward}
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale
  Schuurmans, et~al.
\newblock Reward augmented maximum likelihood for neural structured prediction.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  1723--1731, 2016.

\bibitem[\protect\citeauthoryear{Owen}{2013}]{owen2013monte}
Art~B. Owen.
\newblock {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[\protect\citeauthoryear{Peters \bgroup \em et al.\egroup
  }{2010}]{peters2010relative}
Jan Peters, Katharina M{\"u}lling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In {\em AAAI}, pages 1607--1612. Atlanta, 2010.

\bibitem[\protect\citeauthoryear{Schulman \bgroup \em et al.\egroup
  }{2015}]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[\protect\citeauthoryear{Schulman \bgroup \em et al.\egroup
  }{2017a}]{schulman2017equivalence}
John Schulman, Xi~Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock {\em arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[\protect\citeauthoryear{Schulman \bgroup \em et al.\egroup
  }{2017b}]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[\protect\citeauthoryear{Silver \bgroup \em et al.\egroup
  }{2016}]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489, 2016.

\bibitem[\protect\citeauthoryear{Sutton \bgroup \em et al.\egroup
  }{1998}]{sutton1998reinforcement}
Richard~S Sutton, Andrew~G Barto, et~al.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 1998.

\bibitem[\protect\citeauthoryear{Tangkaratt \bgroup \em et al.\egroup
  }{2017}]{tangkaratt2017guide}
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama.
\newblock Guide actor-critic for continuous control.
\newblock {\em arXiv preprint arXiv:1705.07606}, 2017.

\bibitem[\protect\citeauthoryear{Todorov \bgroup \em et al.\egroup
  }{2012}]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pages 5026--5033. IEEE, 2012.

\bibitem[\protect\citeauthoryear{Van~Hoof \bgroup \em et al.\egroup
  }{2015}]{van2015learning}
Herke Van~Hoof, Jan Peters, and Gerhard Neumann.
\newblock Learning of non-parametric control policies with high-dimensional
  state features.
\newblock In {\em Artificial Intelligence and Statistics}, pages 995--1003,
  2015.

\bibitem[\protect\citeauthoryear{Williams and
  Peng}{1991}]{williams1991function}
Ronald~J Williams and Jing Peng.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock {\em Connection Science}, 3(3):241--268, 1991.

\bibitem[\protect\citeauthoryear{Williams}{1992}]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 8(3-4):229--256, 1992.

\end{thebibliography}
