@article{schulman2017proximal,
	title={Proximal policy optimization algorithms},
	author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	journal={arXiv preprint arXiv:1707.06347},
	year={2017}
}

@article{lillicrap2015continuous,
	title={Continuous control with deep reinforcement learning},
	author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	journal={arXiv preprint arXiv:1509.02971},
	year={2015}
}

@book{sutton1998reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G and others},
	year={1998},
	publisher={MIT press}
}

@article{haarnoja2017reinforcement,
	title={Reinforcement learning with deep energy-based policies},
	author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	journal={arXiv preprint arXiv:1702.08165},
	year={2017}
}

@article{schulman2017equivalence,
	title={Equivalence between policy gradients and soft q-learning},
	author={Schulman, John and Chen, Xi and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1704.06440},
	year={2017}
}

@article{fox2015taming,
	title={Taming the noise in reinforcement learning via soft updates},
	author={Fox, Roy and Pakman, Ari and Tishby, Naftali},
	journal={arXiv preprint arXiv:1512.08562},
	year={2015}
}

@inproceedings{van2015learning,
	title={Learning of non-parametric control policies with high-dimensional state features},
	author={Van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
	booktitle={Artificial Intelligence and Statistics},
	pages={995--1003},
	year={2015}
}

@inproceedings{peters2010relative,
	title={Relative Entropy Policy Search.},
	author={Peters, Jan and M{\"u}lling, Katharina and Altun, Yasemin},
	booktitle={AAAI},
	pages={1607--1612},
	year={2010},
	organization={Atlanta}
}

@article{tangkaratt2017guide,
	title={Guide Actor-Critic for Continuous Control},
	author={Tangkaratt, Voot and Abdolmaleki, Abbas and Sugiyama, Masashi},
	journal={arXiv preprint arXiv:1705.07606},
	year={2017}
}

@inproceedings{thomas2013projected,
	title={Projected natural actor-critic},
	author={Thomas, Philip S and Dabney, William C and Giguere, Stephen and Mahadevan, Sridhar},
	booktitle={Advances in neural information processing systems},
	pages={2337--2345},
	year={2013}
}

@article{mahadevan2012sparse,
	title={Sparse Q-learning with mirror descent},
	author={Mahadevan, Sridhar and Liu, Bo},
	journal={arXiv preprint arXiv:1210.4893},
	year={2012}
}

@inproceedings{liu2015finite,
	title={Finite-Sample Analysis of Proximal Gradient TD Algorithms.},
	author={Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
	booktitle={UAI},
	pages={504--513},
	year={2015},
	organization={Citeseer}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{williams1992simple,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Kluwer Academic Publishers}
}

@article{williams1991function,
  title={Function optimization using connectionist reinforcement learning algorithms},
  author={Williams, Ronald J and Peng, Jing},
  journal={Connection Science},
  volume={3},
  number={3},
  pages={241--268},
  year={1991},
  publisher={Taylor \& Francis}
}

@article{thrun1992efficient,
  title={Efficient exploration in reinforcement learning},
  author={Thrun, Sebastian B},
  journal={Technical report},
  year={1992}
}

@inproceedings{nachum2017improving,
  title={Improving policy gradient by exploring under-appreciated rewards},
  author={Nachum, Ofir and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2772--2782},
  year={2017}
}

@article{nemirovskii1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskii, Arkadii and Yudin, David Borisovich and Dawson, Edgar Ronald},
  year={1983},
  publisher={Wiley}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{peters2007reinforcement,
  title={Reinforcement learning by reward-weighted regression for operational space control},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={745--750},
  year={2007},
  organization={ACM}
}

@inproceedings{wierstra2008episodic,
  title={Episodic reinforcement learning by logistic reward-weighted regression},
  author={Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  booktitle={International Conference on Artificial Neural Networks},
  pages={407--416},
  year={2008},
  organization={Springer}
}

@inproceedings{montgomery2016guided,
  title={Guided policy search via approximate mirror descent},
  author={Montgomery, William H and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4008--4016},
  year={2016}
}

@inproceedings{nachum2017trust,
  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={ICLR},
  year={2017}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, Dave},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@inproceedings{abdolmaleki2018maximum,
  title={Maximum a Posteriori Policy Optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  booktitle={ICLR},
  year={2018}
}

@book{kevin2012machine, 
title = {Machine learning: a probabilistic perspective}, 
author = {Kevin P Murphy}, 
year = {2012}, 
address = {Cambridge, MA} 
}

@inproceedings{norouzi2016reward,
  title={Reward augmented maximum likelihood for neural structured prediction},
  author={Norouzi, Mohammad and Bengio, Samy and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale and others},
  booktitle={Advances In Neural Information Processing Systems},
  pages={1723--1731},
  year={2016}
}

@inproceedings{ding2017cold,
  title={Cold-Start Reinforcement Learning with Softmax Policy Gradient},
  author={Ding, Nan and Soricut, Radu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2817--2826},
  year={2017}
}

@book{owen2013monte,
  title={Monte Carlo theory, methods and examples},
  author={Owen, Art B.},
  year={2013}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{levine2018reinforcement,
  title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@article{greensmith2004variance,
  title={Variance reduction techniques for gradient estimates in reinforcement learning},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Nov},
  pages={1471--1530},
  year={2004}
}
