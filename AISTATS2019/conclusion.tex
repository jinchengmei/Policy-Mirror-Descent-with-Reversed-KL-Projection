
\section{Conclusion and Future Work}
\label{sec:conclusion_and_future_work}

We have proposed reversed entropy policy mirror descent (REPMD)
as an effective new approach for policy based reinforcement learning
that also guarantees monotonic improvement in a well motivated objective.
We show that the resulting method achieves better exploration than both
a directed exploration strategy (UREX) and undirected maximum entropy
exploration (MENT). 
It will be interesting to further extend the follow-on
PMAC actor-critic framework
with further development of the value function learning approach.
