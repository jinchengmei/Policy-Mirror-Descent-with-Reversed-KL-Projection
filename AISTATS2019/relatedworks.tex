
\section{Related Work}
\label{sec:related_work}

The lift-and-project approach is distinct from the previous literature on policy
search, with the exception of a few recent works:
Mirror Descent Guided Policy Search (MDGPS) \citep{montgomery2016guided},
Guide Actor-Critic (GAC) \citep{tangkaratt2017guide},
Maximum a-posteriori Policy Optimisation (MPO) \citep{abdolmaleki2018maximum},
and Soft Actor-Critic (SAC) \citep{haarnoja2018soft}.
These approaches also adopt a mirror descent framework,
but differ from the proposed approach in key aspects.
%
MDGPS \citep{montgomery2016guided} follows a different learning principle,
using the Lift Step to learn multiple local policies 
(rather than a single policy)
then aligning these with a global policy in the Project Step.
MDGPS also does not include the entropy term in the Lift objective,
which we have found to be essential for exploration. 
%
MPO \citep{abdolmaleki2018maximum} also neglects to add the additional entropy 
term;
\cref{subsec:ablationstudy} shows that entropy regularization with an
appropriate annealing of $\tau'$ significantly improves learning efficiency.
%
Both GAC and SAC use the mode seeking direction of KL divergence in
the Project step, in opposition to the mean seeking direction
we consider here \citep{tangkaratt2017guide,haarnoja2018soft}. 
Additionally, 
GAC seeks to match the mean of Gaussian policies under second order approximation in the Project step,
instead of directly minimizing the KL divergence with gradient descent\citep{tangkaratt2017guide}.
The Gaussian assumption also restricts GAC to the continuous action setting.
SAC only uses entropy in the Lift Step, neglecting the proximal relative entropy.
The benefits of regularizing with relative entropy has been discussed in TRPO \citep{schulman2015trust} and MPO \citep{abdolmaleki2018maximum},
where it is noted that proximal regularization
significantly improves learning stability.
As shown in \cref{subsec:compeval}, the performance of SAC heavily relies on the re-parametrization trick, which again restricts SAC to the continuous setting.
%Moreover, REPMD is a purely policy-based method, which can be applied to the setting where only cumulative reward is available, and thus actor-critic algorithms can not be applied. 
Moreover, 
SAC can also be modified to be policy based  if necessary \footnote{for example when only cumulative reward is available}.
In such case, SAC will degenerates to MENT \citep{levine2018reinforcement}, which is outperformed by REPMD as shown in \cref{subsec:compeval}.
Although one might also attempt to interpret ``one-step'' methods
in terms of lift-and-project,
these approaches would obliviously still differ from REPMD,
given that we use different directions of the KL divergence
for the Lift and Project steps respectively. 

Regarding the optimization objective,
several existing methods have considered related approaches,
either by considering (relative) entropy regularization during policy search,
or directly using KL divergence as the target objective. 
As noted in \cref{subsec:revisitTRPO},
REPMD resembles policy gradient methods that maximize expected reward
with an additional entropy regularizer
\citep{williams1991function,fox2015taming,nachum2017bridging}.
Using KL divergence (or Bregman divergences more generally)
as regularizers has also been explored
in \citet{liu2015finite,thomas2013projected,mahadevan2012sparse}.
However, these approaches differ from the proposed method
in important ways.
In particular, they apply regularization to the parameters of the
\emph{linear} approximated value functions, whereas here KL regularization
is applied directly to the policy space.
The literature on relative entropy policy search also uses a similar
KL divergence regularization scheme
\citep{peters2010relative,van2015learning},
but on joint state-action distributions.
Instead of KL divergence, Reward-Weighted Regression (RWR) uses
a log of the correlation between $\pi^*_\tau$ and $\pi_\theta$,
which is then approximated similar to a cross entropy loss
\citep{peters2007reinforcement,wierstra2008episodic}.

TRPO and PPO also have a similar formulation to
\cref{eq:max_expected_reward_plus_relative_entropy},
using a constrained version with a mean seeking KL divergence
\cite{schulman2015trust,schulman2017proximal}. 
Our proposed method includes additional modifications that,
as shown in \cref{sec:experiments}, significantly improve performance.
UREX also uses mean seeking KL for regularization,
which encourages exploration but also complicates the optimization;
as shown in \cref{sec:experiments},
UREX is significantly less efficient than the method proposed here.

Trust PCL adopts the same objective defined in \cref{eq:repmd},
including both entropy and relative entropy regularization
\citep{nachum2017trust}.
However, the policy update strategy is substantially different:
while REPMD uses KL projection, Trust PCL
minimizes a path inconsistency error (inherited from PCL)
between the value and policy along observed trajectories
\citep{nachum2017bridging}.
Although policy optimization by minimizing path inconsistency error
can efficiently utilize off-policy data, this approach loses the 
desirable monotonic improvement guarantee.

In terms of existing theoretical analyses, 
similar monotonic improvement guarantee exists for TRPO, but only for an
impractical formulation.%
%
\footnote{
	For the monotonic improvement guarantee,
	TRPO must use $\KL^{\max} $ rather than the stanard KL divergence.
} 
Here, by contrast,
we use the lift-and-project reformulation to establish a monotonic
improvement guarantee in a simple and direct way.
MPO also enjoys a monotonic
improvement guarantee but only on the non-parametric policy from the Lift step.
SAC obtains a similar result with respect to the optimal achievable
Q-values.
All these results are obtained under the solvability assumption that a non-convex optimization problem can be solved globally.
Furthermore, under the same assumption SAC is also shown to obtain a global optimum property. 
Such property, while stronger than our fixed points guarantee at first sight, arguably is essentially a natural corollary from the solvability assumption.
One way to see it is to consider its policy-based version.
As mentioned above, the policy-based version of SAC degenerates to MENT,
and this property becomes a tautology: assume that $\min_\pi \KL(\pi \| \pi^*_\tau)$ can be solved globally, then $\pi^*$ maximizes $\E_{\rho \sim \pi} r(\rho) + \tau \cH(\pi)$  globally.
%We have shown that similar results hold in the case of PMD/REPMD,
In this paper, we have established something stronger:
when the projection step cannot be efficiently solved to global optimality,
we have shown that PMD/REPMD still preserve stationary points in 
their respective, principled objectives.
MPO and SAC have no such guarantee, as far as we are aware. 

